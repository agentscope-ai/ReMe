---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Agent Retrieve

## AgenticRetrieveOp

### Purpose

A ReAct (Reasoning + Acting) agent that answers user queries by interacting with available tools and managing conversation context intelligently. The agent can perform multi-turn reasoning, retrieve information from external sources, and maintain efficient context usage through automatic compression and compaction.

### Functionality

- Implements the ReAct pattern: alternating between reasoning about the task and taking actions through tool calls
- Automatically selects and invokes appropriate tools (e.g., `Grep`, `ReadFile`) based on user queries
- Supports intelligent context management to handle long conversations and large tool outputs
- Integrates seamlessly with context offloading mechanisms (compact/compress/auto modes)
- Provides structured responses with reasoning traces and final answers
- Maintains conversation history while optimizing token usage
- Handles multi-step tasks that require sequential tool invocations

### Parameters

- `messages` (array, **required**):
  - List of conversation messages that form the dialogue history
  - Each message should include `role` (system/user/assistant/tool) and `content`
  - Assistant messages may include `tool_calls` for agent actions
  - Tool messages contain results from previous tool invocations
  - Example message structure:
    ```json
    {
      "role": "user",
      "content": "How to become smarter?"
    }
    ```

- `context_manage_mode` (string, **required**):
  - Context management strategy for handling token limits
  - `"compact"`: Stores large tool message content in external files, keeping previews in context
  - `"compress"`: Uses LLM to summarize older conversation segments
  - `"auto"`: Applies compaction first, then compression if the compaction ratio exceeds threshold
  - Allowed values: `["compact", "compress", "auto"]`
  - Recommended: `"auto"` for balanced performance and token efficiency

- `max_total_tokens` (integer, optional, default: `20000`):
  - Maximum token count threshold that triggers context management operations
  - For compaction mode: total token count across all messages
  - For compression mode: excludes `keep_recent_count` messages and system messages
  - Agent will manage context automatically when this threshold is exceeded
  - Adjust based on your LLM's context window size

- `max_tool_message_tokens` (integer, optional, default: `2000`):
  - Maximum token count for individual tool message before compaction applies
  - Tool responses exceeding this limit will be stored externally with a file reference
  - Only a preview or summary remains in the conversation context
  - Useful for handling large file reads or extensive search results

- `group_token_threshold` (integer, optional):
  - Maximum token count per message group when applying LLM-based compression
  - If `None` or `0`, all eligible messages are compressed together in one group
  - Messages individually exceeding this threshold form their own compression group
  - Only applies in `"compress"` or `"auto"` modes
  - Helps balance compression granularity and efficiency

- `keep_recent_count` (integer, optional, default: `1` for compaction, `2` for compression):
  - Number of most recent messages to preserve without modification
  - These messages remain in full to maintain immediate conversation context
  - System messages are always preserved regardless of this setting
  - Higher values retain more recent context but use more tokens

- `store_dir` (string, optional):
  - Directory path for storing offloaded content from tool messages and compressed groups
  - Full tool message content and compression summaries are saved as `.txt` files in this directory
  - File references are embedded in the context for agent retrieval
  - Required for compaction and compression operations to function
  - Example: `/Users/username/project/cache_file`

- `chat_id` (string, optional):
  - Unique identifier for the current chat session
  - Used for organizing and naming stored files in `store_dir`
  - If not provided, a UUID will be automatically generated
  - Helps track and manage multiple concurrent sessions
  - Example: `"research_session_001"`

### Return Value

The operation returns a structured response containing:

- `answer` (string): The final response generated by the agent, which may include reasoning and formatted output (e.g., JSON, markdown)
- `messages` (array): The processed conversation history with context management applied (compacted tool messages, compressed history)
- `success` (boolean): Indicates whether the agent successfully completed the task
- `metadata` (dict): Additional information about the execution, such as token usage, tool invocations, and file references

**Example Response:**

After processing a user query "Please elaborate more on the four-detail observation practice and reply in JSON format", the agent:

1. Uses `Grep` to search for relevant content in stored files
2. Reads the retrieved file using `ReadFile`
3. Generates a structured JSON response explaining the memory training method

```json
{
  "answer": "```json\n{\n  \"method\": \"四细节观察练习\",\n  \"description\": \"这是一种通过在公众场合主动观察他人或事物的细节来训练大脑灵敏度的方法...\",\n  \"steps\": [...],\n  \"benefits\": [...]\n}\n```",
  "messages": [],
  "success": true,
  "metadata": {}
}
```

### Usage Example

```python
from reme_ai.agent.react import AgenticRetrieveOp

# Initialize the agent
agent = AgenticRetrieveOp()

# Prepare conversation with context management
payload = {
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant. Always use grep before reading files."
        },
        {
            "role": "user",
            "content": "How to become smarter?"
        },
        # ... additional conversation turns with tool calls and results
        {
            "role": "user",
            "content": "Please elaborate on the four-detail observation practice and reply in JSON format"
        }
    ],
    "context_manage_mode": "compact",
    "max_total_tokens": 200,
    "max_tool_message_tokens": 200,
    "keep_recent_count": 3,
    "store_dir": "/path/to/cache_file",
    "chat_id": "research_session_001"
}

# Execute the agent
result = agent.execute(payload)

# Access results
print(result["answer"])  # Agent's final response
print(result["success"])  # True if successful
```



### Notes

- The agent automatically manages tool selection and invocation based on the conversation context
- Context management is crucial for long conversations to avoid exceeding token limits
- Stored files in `store_dir` can be reused across sessions when referenced by the agent
- The agent follows a ReAct pattern: it reasons about what to do next, takes action through tools, observes results, and continues until the task is complete
- For more implementation details, please refer to: [test/test_agent_retrieve.py](file://../test/test_agent_retrieve.py)
